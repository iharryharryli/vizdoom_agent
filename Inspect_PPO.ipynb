{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import algo\n",
    "from arguments import get_args\n",
    "from envs import make_vec_envs_ViZDoom\n",
    "from model import Policy\n",
    "from storage import RolloutStorage\n",
    "from utils import get_vec_normalize\n",
    "from visualize import visdom_plot\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"../vizdoom_record2/\"\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "reward_history = os.path.join(result_dir, \"reward_history\")\n",
    "loss_history = os.path.join(result_dir, \"loss_history\")\n",
    "\n",
    "#remove old record files\n",
    "for f in [reward_history, loss_history]:\n",
    "    try:\n",
    "        os.remove(f)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "\n",
    "def ppo_hyper():\n",
    "    args.algo = \"ppo\"\n",
    "    args.use_gae = False\n",
    "    args.tau = 0.95\n",
    "    args.lr = 2.5e-4\n",
    "    args.value_loss_coef = 1.0\n",
    "    args.clip_param = 0.1\n",
    "    args.num_processes = 8\n",
    "    args.num_steps = 512\n",
    "    args.num_mini_batch = 1\n",
    "    args.gamma = 1.0\n",
    "    args.entropy_coef = 0.0\n",
    "    args.ppo_epoch = 1\n",
    "\n",
    "def a2c_hyper():\n",
    "    args.algo = \"a2c\"\n",
    "    args.gamma = 1.0\n",
    "    args.num_steps = 512\n",
    "    args.num_processes = 8\n",
    "    args.entropy_coef = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_hyper()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = int(args.num_frames) // args.num_steps // args.num_processes\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "envs = make_vec_envs_ViZDoom(args.seed, args.num_processes, device)\n",
    "\n",
    "actor_critic = Policy(envs.observation_space.shape, envs.action_space,\n",
    "    base_kwargs={'recurrent': args.recurrent_policy})\n",
    "actor_critic.to(device)\n",
    "\n",
    "if args.algo == 'a2c':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                               args.entropy_coef, lr=args.lr,\n",
    "                               eps=args.eps, alpha=args.alpha,\n",
    "                               max_grad_norm=args.max_grad_norm)\n",
    "else:\n",
    "    agent = algo.PPO(actor_critic, args.clip_param, args.ppo_epoch, args.num_mini_batch,\n",
    "                         args.value_loss_coef, args.entropy_coef, lr=args.lr,\n",
    "                               eps=args.eps,\n",
    "                               max_grad_norm=args.max_grad_norm)\n",
    "\n",
    "rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                    envs.observation_space.shape, envs.action_space,\n",
    "                    actor_critic.recurrent_hidden_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "recent_count = 50\n",
    "episode_rewards = deque(maxlen=recent_count)\n",
    "episode_lengths = deque(maxlen=recent_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc_step = []\n",
    "acc_reward = []\n",
    "acc_length = []\n",
    "\n",
    "for j in range(num_updates):\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'Episode_Total_Reward' in info.keys():\n",
    "                episode_rewards.append(info['Episode_Total_Reward'])\n",
    "            if 'Episode_Total_Len' in info.keys():\n",
    "                episode_lengths.append(info['Episode_Total_Len'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(rollouts.obs[-1],\n",
    "                                            rollouts.recurrent_hidden_states[-1],\n",
    "                                            rollouts.masks[-1]).detach()\n",
    "\n",
    "    rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "    rollouts.after_update()\n",
    "    \n",
    "    total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "    \n",
    "    with open(loss_history, 'a') as the_file:\n",
    "        the_file.write(\"{} {} {} {} \\n\".format(total_num_steps, value_loss, action_loss, dist_entropy))\n",
    "    \n",
    "    if len(episode_rewards) > 0:\n",
    "        print(\"{} updates: avg reward = {}, avg length = {}\".format(total_num_steps, np.mean(episode_rewards),\n",
    "                                                               np.mean(episode_lengths)))\n",
    "        \n",
    "        with open(reward_history, 'a') as the_file:\n",
    "            the_file.write('{} {} {} \\n'.format(total_num_steps, np.mean(episode_rewards),\n",
    "                                               np.mean(episode_lengths)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = os.path.join(result_dir, \"model.save\")\n",
    "torch.save(actor_critic.state_dict(), MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
